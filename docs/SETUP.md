# ğŸ³ Setup TÃ©cnico - Kamin Data Challenge

GuÃ­a completa para configurar el entorno Docker y ejecutar el anÃ¡lisis de datos financieros.

## ğŸš€ Inicio RÃ¡pido

### OpciÃ³n 1: Script automatizado (Recomendado)
```bash
# Iniciar Jupyter Lab
./scripts/start-dev.sh up

# Ver en navegador: http://localhost:8889
```

### OpciÃ³n 2: Docker Compose manual
```bash
# Iniciar contenedor
docker-compose up --build

# Detener contenedor  
docker-compose down
```

## ğŸ“‹ Comandos Disponibles

El script `./scripts/start-dev.sh` incluye varios comandos Ãºtiles:

```bash
./scripts/start-dev.sh up      # Iniciar Jupyter Lab (puerto 8889)
./scripts/start-dev.sh down    # Detener contenedores
./scripts/start-dev.sh build   # Reconstruir imagen Docker
./scripts/start-dev.sh shell   # Abrir shell interactivo
./scripts/start-dev.sh clean   # Limpiar contenedores e imÃ¡genes
./scripts/start-dev.sh logs    # Ver logs del contenedor
./scripts/start-dev.sh help    # Mostrar ayuda
```

## ğŸ”§ ConfiguraciÃ³n del Entorno

### Contenedor Principal
- **Base**: Python 3.9-slim
- **Puerto**: 8889 (Jupyter Lab) - *Evita conflictos con puerto 8888*
- **Usuario**: kamin (no-root para seguridad)
- **Directorio de trabajo**: `/app`

### Dependencias Incluidas
- **AnÃ¡lisis de datos**: pandas, numpy, duckdb
- **VisualizaciÃ³n**: matplotlib, seaborn, plotly
- **Desarrollo**: Jupyter Lab, IPython widgets
- **Herramientas**: black, flake8, pytest

### VolÃºmenes Montados
```
./src        â†’ /app/src        (cÃ³digo fuente)
./notebooks  â†’ /app/notebooks  (notebooks Jupyter)
./data       â†’ /app/data       (datasets)
./queries    â†’ /app/queries    (consultas SQL)
```

## ğŸ“Š Notebooks Disponibles

### `sql_analytics.ipynb`
Notebook principal del data challenge que incluye:
- âœ… ConfiguraciÃ³n de SQLAnalytics
- ğŸ§ª GeneraciÃ³n de tablas analÃ­ticas
- ğŸ“ˆ VerificaciÃ³n de resultados
- ğŸ® Pipeline completo de anÃ¡lisis

### Uso del Notebook
1. Ejecutar `./scripts/start-dev.sh up`
2. Abrir http://localhost:8889
3. Navegar a `notebooks/sql_analytics.ipynb`
4. Ejecutar las celdas secuencialmente

## ğŸ› ï¸ Desarrollo

### Estructura del Proyecto
```
kamin_data_challenge/
â”œâ”€â”€ Dockerfile                 # DefiniciÃ³n del contenedor
â”œâ”€â”€ docker-compose.yml         # OrquestaciÃ³n de servicios
â”œâ”€â”€ scripts/start-dev.sh      # Script de desarrollo
â”œâ”€â”€ requirements.txt          # Dependencias Python
â”œâ”€â”€ src/                      # CÃ³digo fuente
â”‚   â”œâ”€â”€ load.py              # Funciones de carga
â”‚   â”œâ”€â”€ transform.py         # Funciones de transformaciÃ³n
â”‚   â””â”€â”€ sql_analytics.py     # Motor SQL Analytics
â”œâ”€â”€ queries/DML/             # Consultas SQL optimizadas
â”‚   â”œâ”€â”€ client_summary.sql   # AnÃ¡lisis por cliente
â”‚   â””â”€â”€ event_time_series.sql # Series temporales
â”œâ”€â”€ notebooks/               # Notebooks Jupyter
â”‚   â””â”€â”€ sql_analytics.ipynb  # AnÃ¡lisis principal
â””â”€â”€ data/                    # Datasets
    â”œâ”€â”€ raw/                 # Datos originales
    â”œâ”€â”€ processed/           # Datos procesados
    â””â”€â”€ analytics/           # Resultados analÃ­ticos
```

### Desarrollo en Tiempo Real
Los cambios en el cÃ³digo fuente se reflejan inmediatamente en el contenedor gracias a los volÃºmenes montados:

- Edita archivos en `./src/` â†’ Cambios disponibles en `/app/src/`
- Crea notebooks en `./notebooks/` â†’ Accesibles en Jupyter Lab
- Modifica consultas en `./queries/` â†’ Disponibles inmediatamente
- Agrega datos en `./data/` â†’ Disponibles para anÃ¡lisis

## ğŸ” Funciones Principales

### SQL Analytics Engine (`src/sql_analytics.py`)
```python
from src.sql_analytics import SQLAnalytics

# Inicializar motor analÃ­tico
analytics = SQLAnalytics(processed_data_path, output_path)

# Generar todas las tablas
results = analytics.generate_analytics_tables()

# O crear tablas especÃ­ficas
analytics.create_client_summary()
analytics.create_event_time_series()
```

### Carga de Datos (`src/load.py`)
```python
from load import load_csv

# Cargar un CSV especÃ­fico
df = load_csv('clients.csv', base_path=DATA_RAW)
```

### Transformaciones (`src/transform.py`)
```python
from transform import normalize_retry_logs_metadata

# Normalizar retry logs
df_clean = normalize_retry_logs_metadata(df_retries)
```

## ğŸ“ Ejemplos de Uso

### Shell Interactivo
```bash
# Acceder al shell del contenedor
./scripts/start-dev.sh shell

# Dentro del contenedor
python -c "import pandas as pd; print('âœ… Pandas disponible')"
python src/sql_analytics.py  # Ejecutar pipeline completo
```

### Comandos Python en Jupyter
```python
# Configurar path
import sys
sys.path.append('/app/src')

# Importar componentes
from sql_analytics import SQLAnalytics
from pathlib import Path

# Ejecutar anÃ¡lisis
analytics = SQLAnalytics(
    Path('/app/data/processed'), 
    Path('/app/data/analytics')
)
results = analytics.generate_analytics_tables()
```

## ğŸ› Troubleshooting

### Error: Puerto 8889 en uso
```bash
# Cambiar puerto en docker-compose.yml
ports:
  - "8890:8888"  # Usar puerto 8890 en lugar de 8889
```

### Error: Permisos de archivos
```bash
# Dar permisos al script
chmod +x scripts/start-dev.sh

# Resetear permisos del proyecto
sudo chown -R $USER:$USER .
```

### Error: Datos no encontrados
```bash
# Verificar estructura de datos
ls -la data/raw/
# Debe contener: clients.csv, events.csv, retry_logs.csv

ls -la data/processed/
# Debe contener los CSVs procesados
```

### Error: Consultas SQL no encontradas
```bash
# Verificar queries SQL
ls -la queries/DML/
# Debe contener: client_summary.sql, event_time_series.sql
```

### Reconstruir completamente
```bash
# Limpiar todo y reconstruir
./scripts/start-dev.sh clean
./scripts/start-dev.sh build
./scripts/start-dev.sh up
```

### Problemas de sincronizaciÃ³n de archivos
```bash
# Verificar volÃºmenes montados
docker exec kamin_data_challenge ls -la /app/queries/DML/
docker exec kamin_data_challenge cat /app/queries/DML/client_summary.sql | head -5
```

## ğŸ¯ Ventajas del Entorno Docker

1. **ğŸ”’ Aislamiento**: No interfiere con otros proyectos
2. **ğŸ“¦ Reproducibilidad**: Mismo entorno en cualquier mÃ¡quina
3. **âš¡ Rapidez**: Setup instantÃ¡neo
4. **ğŸ›¡ï¸ Seguridad**: Usuario no-root
5. **ğŸ”„ Persistencia**: Datos y configuraciÃ³n guardados
6. **ğŸ¨ Completo**: Todas las herramientas incluidas
7. **ğŸ DuckDB**: Motor SQL optimizado para anÃ¡lisis

## ğŸš€ Performance Tips

### Para datasets grandes
- DuckDB maneja eficientemente datasets de varios GB
- Las consultas SQL estÃ¡n optimizadas con CTEs
- Los CAST explÃ­citos evitan errores de tipos

### Para desarrollo
- Usa `./scripts/start-dev.sh logs` para debugging
- Los volÃºmenes sincronizados permiten editar sin reconstruir
- El logging detallado ayuda a rastrear errores

## ğŸ“ Soporte

Si tienes problemas:
1. Verifica que Docker estÃ© corriendo: `docker --version`
2. Revisa los logs: `./scripts/start-dev.sh logs`
3. Prueba reconstruir: `./scripts/start-dev.sh build`
4. Limpia y reinicia: `./scripts/start-dev.sh clean && ./scripts/start-dev.sh up`
5. Verifica puertos: `netstat -tlnp | grep 8889`

---

Â¡Listo para analizar datos financieros como un pro! ğŸš€ğŸ“Š 