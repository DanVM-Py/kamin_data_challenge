{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports completados\n",
      "ğŸ“ Data procesada: ../data/processed\n",
      "ğŸ“ Output analytics: ../data/analytics\n",
      "âœ… clients.csv encontrado\n",
      "âœ… events.csv encontrado\n",
      "âœ… retry_logs.csv encontrado\n",
      "\n",
      "ğŸ¯ Â¡Listo para SQLAnalytics!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ Imports y configuraciÃ³n\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Agregar src al path para importar SQLAnalytics\n",
    "sys.path.append('../src')\n",
    "from sql_analytics import SQLAnalytics\n",
    "\n",
    "print(\"âœ… Imports completados\")\n",
    "\n",
    "# ğŸ“ Configurar paths\n",
    "PROCESSED_DATA = Path('../data/processed')\n",
    "ANALYTICS_OUTPUT = Path('../data/analytics')\n",
    "\n",
    "print(f\"ğŸ“ Data procesada: {PROCESSED_DATA}\")\n",
    "print(f\"ğŸ“ Output analytics: {ANALYTICS_OUTPUT}\")\n",
    "\n",
    "# Verificar que existen los CSVs procesados\n",
    "csv_files = ['clients.csv', 'events.csv', 'retry_logs.csv']\n",
    "for csv_file in csv_files:\n",
    "    file_path = PROCESSED_DATA / csv_file\n",
    "    if file_path.exists():\n",
    "        print(f\"âœ… {csv_file} encontrado\")\n",
    "    else:\n",
    "        print(f\"âŒ {csv_file} NO encontrado\")\n",
    "\n",
    "print(\"\\nğŸ¯ Â¡Listo para SQLAnalytics!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sql_analytics:ğŸš€ SQLAnalytics inicializado\n",
      "INFO:sql_analytics:ğŸ“ Input: ../data/processed\n",
      "INFO:sql_analytics:ğŸ“ Output: ../data/analytics\n",
      "INFO:sql_analytics:ğŸ“„ Queries: /app/queries/DML\n",
      "INFO:sql_analytics:ğŸ¯ Generando tablas analÃ­ticas...\n",
      "INFO:sql_analytics:ğŸ“¥ Cargando CSVs a tablas SQL...\n",
      "INFO:sql_analytics:âœ… clients: 60 rows cargadas\n",
      "INFO:sql_analytics:âœ… events: 15000 rows cargadas\n",
      "INFO:sql_analytics:âœ… retry_logs: 1500 rows cargadas\n",
      "INFO:sql_analytics:ğŸ“‹ InformaciÃ³n de tablas SQL:\n",
      "INFO:sql_analytics:ğŸ”¨ Creando tabla de resumen de clientes...\n",
      "INFO:sql_analytics:âœ… Query ejecutada: 60 rows generadas\n",
      "INFO:sql_analytics:ğŸ’¾ client_summary.csv: 60 rows exportadas (0.01 MB)\n",
      "INFO:sql_analytics:ğŸ’¾ Tabla client_summary.csv creada exitosamente\n",
      "INFO:sql_analytics:ğŸ”¨ Creando tabla de series temporales de eventos...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ SQLAnalytics inicializado\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š Tabla: clients\n",
      "   Rows: 60\n",
      "   Columns: 6\n",
      "   Schema: client_id, client_name, sector, contract_tier, sign_up_date, notes\n",
      "\n",
      "ğŸ“Š Tabla: events\n",
      "   Rows: 15,000\n",
      "   Columns: 11\n",
      "   Schema: event_id, client_id, type, amount, currency, status, error_code, created_at, completed_at, origin_country, destination_country\n",
      "\n",
      "ğŸ“Š Tabla: retry_logs\n",
      "   Rows: 1,500\n",
      "   Columns: 5\n",
      "   Schema: retry_id, original_event_id, retry_attempt, retry_status, retry_time\n",
      "\n",
      "ğŸ”¨ Creando 2 tablas analÃ­ticas...\n",
      "------------------------------\n",
      "âœ… client_summary.csv: Ã‰XITO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sql_analytics:âœ… Query ejecutada: 3568 rows generadas\n",
      "INFO:sql_analytics:ğŸ’¾ event_time_series.csv: 3568 rows exportadas (0.37 MB)\n",
      "INFO:sql_analytics:ğŸ’¾ Tabla event_time_series.csv creada exitosamente\n",
      "INFO:sql_analytics:ğŸ‰ Â¡Todas las tablas generadas exitosamente!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… event_time_series.csv: Ã‰XITO\n",
      "\n",
      "ğŸ“Š REPORTE FINAL:\n",
      "   âœ… Exitosas: 2/2\n",
      "   âŒ Fallidas: 0/2\n",
      "\n",
      "ğŸ“‹ Resultados detallados:\n",
      "   Client: âœ… Ã‰XITO\n",
      "   Event: âœ… Ã‰XITO\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ Inicializar SQLAnalytics y ejecutar pipeline completo\n",
    "analytics = SQLAnalytics(PROCESSED_DATA, ANALYTICS_OUTPUT)\n",
    "\n",
    "print(\"ğŸ¯ SQLAnalytics inicializado\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Ejecutar pipeline completo con manejo de errores\n",
    "results = analytics.generate_analytics_tables()\n",
    "\n",
    "print(\"\\nğŸ“‹ Resultados detallados:\")\n",
    "for table_type, success in results.items():\n",
    "    status = \"âœ… Ã‰XITO\" if success else \"âŒ FALLO\"\n",
    "    print(f\"   {table_type.capitalize()}: {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kamin Data Challenge",
   "language": "python",
   "name": "kamin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
