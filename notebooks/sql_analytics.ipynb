{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports completados\n",
      "📁 Data procesada: ../data/processed\n",
      "📁 Output analytics: ../data/analytics\n",
      "✅ clients.csv encontrado\n",
      "✅ events.csv encontrado\n",
      "✅ retry_logs.csv encontrado\n",
      "\n",
      "🎯 ¡Listo para SQLAnalytics!\n"
     ]
    }
   ],
   "source": [
    "# 📦 Imports y configuración\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Agregar src al path para importar SQLAnalytics\n",
    "sys.path.append('../src')\n",
    "from sql_analytics import SQLAnalytics\n",
    "\n",
    "print(\"✅ Imports completados\")\n",
    "\n",
    "# 📁 Configurar paths\n",
    "PROCESSED_DATA = Path('../data/processed')\n",
    "ANALYTICS_OUTPUT = Path('../data/analytics')\n",
    "\n",
    "print(f\"📁 Data procesada: {PROCESSED_DATA}\")\n",
    "print(f\"📁 Output analytics: {ANALYTICS_OUTPUT}\")\n",
    "\n",
    "# Verificar que existen los CSVs procesados\n",
    "csv_files = ['clients.csv', 'events.csv', 'retry_logs.csv']\n",
    "for csv_file in csv_files:\n",
    "    file_path = PROCESSED_DATA / csv_file\n",
    "    if file_path.exists():\n",
    "        print(f\"✅ {csv_file} encontrado\")\n",
    "    else:\n",
    "        print(f\"❌ {csv_file} NO encontrado\")\n",
    "\n",
    "print(\"\\n🎯 ¡Listo para SQLAnalytics!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sql_analytics:🚀 SQLAnalytics inicializado\n",
      "INFO:sql_analytics:📁 Input: ../data/processed\n",
      "INFO:sql_analytics:📁 Output: ../data/analytics\n",
      "INFO:sql_analytics:📄 Queries: /app/queries/DML\n",
      "INFO:sql_analytics:🎯 Generando tablas analíticas...\n",
      "INFO:sql_analytics:📥 Cargando CSVs a tablas SQL...\n",
      "INFO:sql_analytics:✅ clients: 60 rows cargadas\n",
      "INFO:sql_analytics:✅ events: 15000 rows cargadas\n",
      "INFO:sql_analytics:✅ retry_logs: 1500 rows cargadas\n",
      "INFO:sql_analytics:📋 Información de tablas SQL:\n",
      "INFO:sql_analytics:🔨 Creando tabla de resumen de clientes...\n",
      "INFO:sql_analytics:✅ Query ejecutada: 60 rows generadas\n",
      "INFO:sql_analytics:💾 client_summary.csv: 60 rows exportadas (0.01 MB)\n",
      "INFO:sql_analytics:💾 Tabla client_summary.csv creada exitosamente\n",
      "INFO:sql_analytics:🔨 Creando tabla de series temporales de eventos...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 SQLAnalytics inicializado\n",
      "==================================================\n",
      "==================================================\n",
      "\n",
      "📊 Tabla: clients\n",
      "   Rows: 60\n",
      "   Columns: 6\n",
      "   Schema: client_id, client_name, sector, contract_tier, sign_up_date, notes\n",
      "\n",
      "📊 Tabla: events\n",
      "   Rows: 15,000\n",
      "   Columns: 11\n",
      "   Schema: event_id, client_id, type, amount, currency, status, error_code, created_at, completed_at, origin_country, destination_country\n",
      "\n",
      "📊 Tabla: retry_logs\n",
      "   Rows: 1,500\n",
      "   Columns: 5\n",
      "   Schema: retry_id, original_event_id, retry_attempt, retry_status, retry_time\n",
      "\n",
      "🔨 Creando 2 tablas analíticas...\n",
      "------------------------------\n",
      "✅ client_summary.csv: ÉXITO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sql_analytics:✅ Query ejecutada: 3568 rows generadas\n",
      "INFO:sql_analytics:💾 event_time_series.csv: 3568 rows exportadas (0.37 MB)\n",
      "INFO:sql_analytics:💾 Tabla event_time_series.csv creada exitosamente\n",
      "INFO:sql_analytics:🎉 ¡Todas las tablas generadas exitosamente!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ event_time_series.csv: ÉXITO\n",
      "\n",
      "📊 REPORTE FINAL:\n",
      "   ✅ Exitosas: 2/2\n",
      "   ❌ Fallidas: 0/2\n",
      "\n",
      "📋 Resultados detallados:\n",
      "   Client: ✅ ÉXITO\n",
      "   Event: ✅ ÉXITO\n"
     ]
    }
   ],
   "source": [
    "# 🚀 Inicializar SQLAnalytics y ejecutar pipeline completo\n",
    "analytics = SQLAnalytics(PROCESSED_DATA, ANALYTICS_OUTPUT)\n",
    "\n",
    "print(\"🎯 SQLAnalytics inicializado\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Ejecutar pipeline completo con manejo de errores\n",
    "results = analytics.generate_analytics_tables()\n",
    "\n",
    "print(\"\\n📋 Resultados detallados:\")\n",
    "for table_type, success in results.items():\n",
    "    status = \"✅ ÉXITO\" if success else \"❌ FALLO\"\n",
    "    print(f\"   {table_type.capitalize()}: {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kamin Data Challenge",
   "language": "python",
   "name": "kamin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
